{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTbi27k1HpK_",
    "outputId": "d6604d63-4c80-4e94-bc5e-0fd21c06afe3"
   },
   "outputs": [],
   "source": [
    "%pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceIAPjMZHqKk"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import random\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE1wybF-Hw52",
    "outputId": "b5a7758b-cb0a-41a6-f69e-996d7513ddb7"
   },
   "outputs": [],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8hZrliVHzSQ"
   },
   "outputs": [],
   "source": [
    "data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBmLDDJ6IQRf"
   },
   "source": [
    "## First, we read the MedMNIST data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8YDE74iICr1",
    "outputId": "ec831ede-4dc4-4710-b51e-dbe3fbc1d120"
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "validation_loader = data.DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AKwZjufIFSE",
    "outputId": "62e3404d-0088-4a99-da73-5960bafa23ed"
   },
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(\"===================\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "zgYO03A9IHy0",
    "outputId": "03637f3b-10c1-4431-d4d7-aac632f0ddd6"
   },
   "outputs": [],
   "source": [
    "# montage\n",
    "\n",
    "train_dataset.montage(length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define seeing to make model results deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the standard ResNet18 network to be compared in performance to the modified network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class StandardNetwork(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Freeze all layers of the original ResNet18 model\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)    # Pass input through ResNet18\n",
    "        return x\n",
    "\n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "network = StandardNetwork(n_classes=n_classes)\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Modified ResNet18 - a simple model for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ExtendedNetwork(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        # super(ExtendedNetwork, self).__init__()\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "       # self.resnet.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Freeze all layers of the original ResNet18 model\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        # self.resnet.fc = nn.Linear(num_features, 512)\n",
    "        self.resnet.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.resnet.fc = nn.Linear(num_features, n_classes)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)    # Pass input through ResNet18\n",
    "        return x\n",
    "    \n",
    "\n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "network = ExtendedNetwork(n_classes=n_classes)\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Optuna for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining modified test function to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(split):\n",
    "    # Load the model with the best accuracy\n",
    "    network.load_state_dict(torch.load('best_model.pth'))\n",
    "    network.eval()\n",
    "    y_true = torch.tensor([], device=device)\n",
    "    y_score = torch.tensor([], device=device)\n",
    "    collected_inputs = torch.tensor([], device=device)\n",
    "    collected_targets = torch.tensor([], device=device)\n",
    "\n",
    "    data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "    if split == \"val\":\n",
    "        data_loader = validation_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32)\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "            else:\n",
    "                targets = targets.squeeze().long()\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "                targets = targets.float().resize_(len(targets), 1).squeeze()\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "            collected_inputs = torch.cat((collected_inputs, inputs), 0)\n",
    "            collected_targets = torch.cat((collected_targets, targets), 0)\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_score = y_score.cpu().detach().numpy()\n",
    "        collected_inputs = collected_inputs.detach().cpu()\n",
    "        collected_targets = collected_targets.detach().cpu()\n",
    "\n",
    "        evaluator = Evaluator(data_flag, split)\n",
    "        auc, acc = evaluator.evaluate(y_score)\n",
    "\n",
    "        # print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "\n",
    "        return (split, auc, acc), y_true, y_score\n",
    "\n",
    "# print('==> Evaluating ...')\n",
    "# train_inputs, train_targets, train_true, train_scores = test('train')\n",
    "# test_inputs, test_targets, test_true, test_scores = test('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftkAQZBGIlvd"
   },
   "source": [
    "## Defining the accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    pred = outputs.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(targets.view_as(pred)).sum().item()\n",
    "    return correct / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(network, valid_loader, criterion, device):\n",
    "    network.eval()  # Set the network to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for validation_inputs, validation_targets in valid_loader:\n",
    "            validation_inputs, validation_targets = validation_inputs.to(device), validation_targets.to(device)\n",
    "            output = network(validation_inputs)\n",
    "            validation_targets = validation_targets.squeeze().long()\n",
    "            loss = criterion(output, validation_targets)\n",
    "            total_loss += loss.item() * validation_inputs.size(0)  # Multiplying by batch size\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Getting the index of the max log-probability\n",
    "            total_correct += pred.eq(validation_targets.view_as(pred)).sum().item()\n",
    "            total_samples += validation_inputs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples * 100  # percentage accuracy\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code for the standard ResNet18 model : StandardNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "train_epoch_accuracies = []\n",
    "test_epoch_accuracies = []\n",
    "\n",
    "best_model_path = None\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-5)\n",
    "\n",
    "# define EPOCHS for optuna\n",
    "NUM_EPOCHS = 20\n",
    "best_validation_loss = float(\"inf\")\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    network.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for tr_inputs, tr_targets in tqdm(train_loader):\n",
    "        tr_inputs = tr_inputs.to(device)\n",
    "        tr_targets = tr_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(tr_inputs)\n",
    "\n",
    "        if task == 'multi-label, binary-class':\n",
    "            tr_targets = tr_targets.to(torch.float32)\n",
    "            loss = criterion(outputs, tr_targets)\n",
    "            predicted = outputs > 0.5\n",
    "        else:\n",
    "            tr_targets = tr_targets.squeeze().long()\n",
    "            loss = criterion(outputs, tr_targets)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct_predictions += (predicted == tr_targets).sum().item()\n",
    "        total_predictions += tr_targets.size(0)\n",
    "        \n",
    "    print(loss)\n",
    "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "    train_epoch_accuracies.append(epoch_accuracy)\n",
    "\n",
    "# validate\n",
    "network.eval()\n",
    "\n",
    "\n",
    "validation_loss, validation_accuracy = validate_model(network, validation_loader, criterion, device)\n",
    "\n",
    "# Outputting the validation loss and accuracy\n",
    "print('Validation - Loss: {:.6f}, Accuracy: {:.2f}%'.format(validation_loss, validation_accuracy))\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code for the MODIFIED ResNet18 model FOR COMPARISON : ExtendedNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0bV0ZYNIjPH",
    "outputId": "bd76d2b8-62b8-4d67-815d-e88a7a56149c"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "train_epoch_accuracies = []\n",
    "test_epoch_accuracies = []\n",
    "\n",
    "best_model_path = None\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    patience = trial.suggest_int('patience', 5, 10)\n",
    "\n",
    "    optimizer = optim.Adagrad(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=patience, threshold=0.0001, threshold_mode=\"abs\")\n",
    "    \n",
    "    # define EPOCHS for optuna\n",
    "    NUM_EPOCHS = 5\n",
    "    best_validation_loss = float(\"inf\")\n",
    "\n",
    "    print(\"TRIAL:\", trial.number)\n",
    "    # train\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        network.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for train_inputs, train_targets in tqdm(train_loader):\n",
    "            train_inputs = train_inputs.to(device)\n",
    "            train_targets = train_targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(train_inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                train_targets = train_targets.to(torch.float32)\n",
    "                loss = criterion(outputs, train_targets)\n",
    "                predicted = outputs > 0.5\n",
    "            else:\n",
    "                train_targets = train_targets.squeeze().long()\n",
    "                loss = criterion(outputs, train_targets)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            correct_predictions += (predicted == train_targets).sum().item()\n",
    "            total_predictions += train_targets.size(0)\n",
    "            \n",
    "        print(loss)\n",
    "        epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "        train_epoch_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # validate\n",
    "    network.eval()\n",
    "\n",
    "\n",
    "    validation_loss, validation_accuracy = validate_model(network, validation_loader, criterion, device)\n",
    "    \n",
    "    # Outputting the validation loss and accuracy\n",
    "    print('Validation - Loss: {:.6f}, Accuracy: {:.2f}%'.format(validation_loss, validation_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(network.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Model saved with validation loss: {validation_loss}\")\n",
    "\n",
    "    \n",
    "    # Scheduler update\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    train_result, train_y_true, train_y_score = test(\"train\")\n",
    "    val_result, val_y_true, val_y_score = test(\"val\")\n",
    "    test_result, test_y_true, test_y_score = test(\"test\")\n",
    "\n",
    "    print('==> Evaluating ...')\n",
    "    test('train')\n",
    "    test('val')\n",
    "    test('test')\n",
    "\n",
    "    print('%s  auc: %.3f  acc:%.3f' % train_result)\n",
    "    print('%s  auc: %.3f  acc:%.3f' % val_result)\n",
    "    print('%s  auc: %.3f  acc:%.3f' % test_result)\n",
    "    print()\n",
    "\n",
    "    # appending the test accuracy on each epoch\n",
    "    test_epoch_accuracies.append(test_result[2])\n",
    "\n",
    "    if test_result[1] >= 0.901 and test_result[2] >= 0.863:\n",
    "        # Updating results_dict and set trial attribute\n",
    "        results_dict = {'Trial': trial.number, 'Split': test_result[0], 'AUC': test_result[1], 'Accuracy': test_result[2], 'Learning Rate': learning_rate, 'Scheduler': scheduler, 'Weight Decay': weight_decay, 'Patience': patience, \"y-true\": test_y_true, \"y-score\": test_y_score}\n",
    "        trial.set_user_attr(\"results_dict\", results_dict)\n",
    "        print(\"Good model found for Trial\", trial.number, \"\\n\")\n",
    "    else:\n",
    "        print(\"Trial\", trial.number, \"did not meet AUC/ACC thresholds.\\n\")\n",
    "\n",
    "\n",
    "     # Saving the results for plotting outside the objective function\n",
    "    trial.set_user_attr(\"epoch_losses\", train_epoch_accuracies)\n",
    "    trial.set_user_attr(\"epoch_accuracies\", test_epoch_accuracies)\n",
    "    \n",
    "    return best_validation_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "print(\"Best trial is :\", trial)\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Parameters: \")\n",
    "for k, v in trial.params.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results to check values retrieved above the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in study.trials:\n",
    "    if \"results_dict\" in trial.user_attrs:\n",
    "        print(f\"Trial {trial.number} Results:\")\n",
    "        print(trial.user_attrs[\"results_dict\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting trial with best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = None\n",
    "max_auc = 0  # Initializing max AUC to the lowest possible value at the start\n",
    "max_acc = 0  # Initializing max Accuracy to the lowest possible value at the starrt\n",
    "\n",
    "for trial in study.trials:\n",
    "    if \"results_dict\" in trial.user_attrs:\n",
    "        # Retrieve AUC and Accuracy from the results_dict\n",
    "        current_auc = trial.user_attrs[\"results_dict\"].get('AUC', 0)\n",
    "        current_acc = trial.user_attrs[\"results_dict\"].get('Accuracy', 0)\n",
    "        # Check if current AUC and Accuracy are greater than the maximum found so far\n",
    "        if current_auc > max_auc or (current_auc == max_auc and current_acc > max_acc):\n",
    "            max_auc = current_auc\n",
    "            max_acc = current_acc\n",
    "            best_trial = trial.user_attrs[\"results_dict\"]\n",
    "\n",
    "print(\"Best Trial:\")\n",
    "print(best_trial)\n",
    "auc = best_trial.get('AUC')\n",
    "acc = best_trial.get('Accuracy')\n",
    "\n",
    "print()\n",
    "print('%s  auc: %.3f  acc:%.3f' % (best_trial.get('Split'), auc, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install torcheval for calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torcheval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_buffer = best_trial.get(\"y-true\")\n",
    "y_score_buffer = best_trial.get(\"y-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MulticlassAUPRC\n",
    "import torch\n",
    "\n",
    "_, train_true, train_scores = test('train')\n",
    "# _, test_true, test_scores = test('test')\n",
    "\n",
    "train_scores = torch.tensor(train_scores, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_true = torch.tensor(train_true, device='cuda' if torch.cuda.is_available() else 'cpu').long().squeeze()  # Ensure labels are long type\n",
    "test_scores = torch.tensor(y_score_buffer, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_true = torch.tensor(y_true_buffer, device='cuda' if torch.cuda.is_available() else 'cpu').long().squeeze()\n",
    "# # Assuming num_classes is the actual number of classes\n",
    "num_classes = 2 # len(torch.unique(torch.tensor(train_true)))  # Update based on your labels\n",
    "\n",
    "# # Initializing the Multiclass AUPRC metric\n",
    "\n",
    "metric_train = MulticlassAUPRC(num_classes=n_classes)\n",
    "metric_train.update(train_scores, train_true)\n",
    "train_auprc_result = metric_train.compute()\n",
    "\n",
    "metric_test = MulticlassAUPRC(num_classes=n_classes)\n",
    "metric_test.update(test_scores, test_true)\n",
    "# # Compute the final AUPRC\n",
    "test_auprc_result = metric_test.compute()\n",
    "\n",
    "print(\"Train AUPRC result:\", train_auprc_result)\n",
    "print(\"Test AUPRC result:\", test_auprc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "train_true = train_true.cpu()\n",
    "train_scores = train_scores.cpu()\n",
    "test_true = test_true.cpu()\n",
    "test_scores = test_scores.cpu()\n",
    "\n",
    "# train_precision = precision_score(train_true, train_scores.argmax(dim=1))\n",
    "test_precision = precision_score(test_true, test_scores.argmax(dim=1))\n",
    "\n",
    "# print(\"Train Precision Score:\", train_precision)\n",
    "print(\"Test Precision Score:\", test_precision)\n",
    "\n",
    "print()\n",
    "\n",
    "# train_recall = recall_score(train_true, train_scores.argmax(dim=1))\n",
    "test_recall = recall_score(test_true, test_scores.argmax(dim=1))\n",
    "\n",
    "# print(\"Train Recall Score:\", train_recall)\n",
    "print(\"Test Recall Score:\", test_recall)\n",
    "\n",
    "print() \n",
    "\n",
    "# train_f1 = f1_score(train_true, train_scores.argmax(dim=1))\n",
    "test_f1 = f1_score(test_true, test_scores.argmax(dim=1))\n",
    "\n",
    "# print(\"Train F1 Score:\", train_f1)\n",
    "print(\"Test F1 Score:\", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_fpr, train_tpr, train_thresholds = metrics.roc_curve(train_true, train_scores[:, 1])\n",
    "train_roc_auc = metrics.auc(train_fpr, train_tpr)\n",
    "\n",
    "test_fpr, test_tpr, test_thresholds = metrics.roc_curve(test_true, test_scores[:, 1])\n",
    "test_roc_auc = metrics.auc(test_fpr, test_tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_fpr, train_tpr, label=f'Train ROC curve (area = {train_roc_auc:.2f})')\n",
    "plt.plot(test_fpr, test_tpr, label=f'Test ROC curve (area = {test_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], \"b--\")  # Dashed diagonal\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_true,  test_scores.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "threshold = 0.5\n",
    "# Assuming test_scores[:, 1] is the probability of the positive class\n",
    "binary_test_scores = (test_scores[:, 1] >= threshold).numpy().astype(int)\n",
    "confusionMatrix = confusion_matrix(test_true.numpy(), binary_test_scores)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = ['Malignant', 'Benign']\n",
    "\n",
    "display_confusion_matrix = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labels)\n",
    "display_confusion_matrix.plot(cmap=\"Blues\", ax=ax)\n",
    "\n",
    "ax.set_xticklabels(['Predicted: Malignant', 'Predicted: Benign'])\n",
    "ax.set_yticklabels(['Actual: Malignant', 'Actual: Benign'])\n",
    "\n",
    "\n",
    "# Adjusting the confusion matrix annotations for TP, FN, FP, TN\n",
    "ax.texts[0].set_text(f'TN: {confusionMatrix[0, 0]}')\n",
    "ax.texts[1].set_text(f'FP: {confusionMatrix[0, 1]}')\n",
    "ax.texts[2].set_text(f'FN: {confusionMatrix[1, 0]}')\n",
    "ax.texts[3].set_text(f'TP: {confusionMatrix[1, 1]}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_model(train_loader, model, criterion, optimizer):\n",
    "    network.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.squeeze().long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate_model(loader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "            all_preds.extend(probabilities.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    auc = roc_auc_score(all_targets, all_preds)\n",
    "    return accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_trial.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Hyperparameters and setup\n",
    "num_epochs = 10\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "# Get the most optimal hyperparameters found when we did the hyperparameter tuning so we can use them here now for K Fold\n",
    "learning_rate_optimal = best_trial.get(\"Learning Rate\")\n",
    "weight_decay_optimal = best_trial.get(\"Weight Decay\")\n",
    "patience_optimal = best_trial.get(\"Patience\")\n",
    "\n",
    "\n",
    "combined_dataset = ConcatDataset([train_dataset, test_dataset, validation_dataset])\n",
    "\n",
    "# Define K Fold Cross Validation Setup\n",
    "k_fold_classifier = KFold(n_splits=k_folds, shuffle=True, random_state=0)\n",
    "\n",
    "accuracies, aucs = [], []\n",
    "\n",
    "for fold, (trainval_idx, test_idx) in enumerate(k_fold_classifier.split(np.arange(len(combined_dataset)))):\n",
    "\n",
    "    # Dividing the data \n",
    "    val_size = len(trainval_idx) // 4  # Since we are rotating this through 4 remaining folds\n",
    "\n",
    "    # We can treate the folds as blocks of equal size\n",
    "    validation_start_idx = val_size * (fold % 4)  # Rotate through the first four chunks\n",
    "    validation_end_idx = validation_start_idx + val_size # End index for validation set\n",
    "\n",
    "    # Create indices for validation and training\n",
    "    val_idx = trainval_idx[validation_start_idx:validation_end_idx]\n",
    "    train_idx = np.concatenate([trainval_idx[:validation_start_idx], trainval_idx[validation_end_idx:]])\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "    test_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "    # Initialize your model and optimizer here\n",
    "    model = ExtendedNetwork(n_classes=2).to(device)\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate_optimal, weight_decay=weight_decay_optimal)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # We train and validate the model now\n",
    "    for epoch in range(num_epochs):\n",
    "        train_model(train_loader, model, criterion, optimizer)\n",
    "        val_acc, val_auc = evaluate_model(val_loader, model, criterion)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_acc, test_auc = evaluate_model(test_loader, model, criterion)\n",
    "    accuracies.append(test_acc)\n",
    "    aucs.append(test_auc)\n",
    "    print(f\"Fold {fold+1}: Test Accuracy = {test_acc:.3f}, Test AUC = {test_auc:.3f}\")\n",
    "\n",
    "# Report average results\n",
    "print(f\"Average Test Accuracy: {np.mean(accuracies):.3f}\")\n",
    "print(f\"Average Test AUC: {np.mean(aucs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Assume 'model' is your trained model\n",
    "network.eval()\n",
    "\n",
    "# For a specific image, we can load it directly as shown below\n",
    "image, label = pil_dataset[108]\n",
    "\n",
    "# Transforming the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    # Include normalization if used during training\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the image to the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image = image.to(device)\n",
    "model = network.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# We assume the output is logits; so we can apply softmax for probabilities\n",
    "probabilities = torch.softmax(output, dim=1).cpu().numpy().flatten()\n",
    "\n",
    "# Assuming class 0 is 'benign' and class 1 is 'malignant'\n",
    "classes = ['benign', 'malignant']\n",
    "predicted_class = classes[probabilities.argmax()]\n",
    "confidence = probabilities.max()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class} with confidence {confidence:.2f}\")\n",
    "\n",
    "\n",
    "image_np = image.squeeze().cpu().numpy()\n",
    "\n",
    "# If the image is grayscale (C, H, W) where C = 1, we convert it to (H, W) for matplotlib\n",
    "if image_np.shape[0] == 1:  # Grayscale image, single channel\n",
    "    image_np = image_np.squeeze(0)  # Now shape is (H, W)\n",
    "elif image_np.shape[0] == 3:  # If it's a 3-channel image\n",
    "    # Convert from (C, H, W) to (H, W, C) for RGB images\n",
    "    image_np = np.transpose(image_np, (1, 2, 0))\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_np, cmap='gray' if image_np.ndim == 2 else None)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "\n",
    "1. add val auc and acc DONE\n",
    "2. k fold DONE\n",
    "3. loss / accuracy graphs\n",
    "4. show basic resnet18\n",
    "5. test on another device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
